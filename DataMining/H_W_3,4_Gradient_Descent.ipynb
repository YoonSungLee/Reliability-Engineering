{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "H.W #3,4. Gradient Descent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOqz3tkYLuSkWOayKgEk+PF"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHFZGc5-2v8g",
        "colab_type": "text"
      },
      "source": [
        "### H.W #3: Logistic Regression에서 손실함수의 도함수가 아래와 같음을 보이시오."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZVAZfAG27ww",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://i.imgur.com/LYDqxId.png\" width=\"100%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pb3JRE-D3RLw",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://i.imgur.com/qTBMszv.jpg\" width=\"100%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tT_T0mhgEOl7",
        "colab_type": "text"
      },
      "source": [
        "### H.W #4: 위의 도함수를 이용해 Gradient Descent 알고리즘으로 beta를 구하시오. 학습률 람다는 0.1로 하고 2000 회 반복 하강한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAV8Uxr7EVT1",
        "colab_type": "code",
        "outputId": "e001756c-90fd-4663-e3e9-6bfac8dcb8c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "x = [1,2,3,4,5,6]\n",
        "y = [0,0,0,1,1,1]\n",
        "n = len(x)\n",
        "b = 3       # random initialization\n",
        "w = 5       # random initialization\n",
        "learning_rate = 0.1\n",
        "epoch = 2000\n",
        "\n",
        "for i in range(epoch):\n",
        "    diff_b = 0\n",
        "    diff_w = 0\n",
        "    for i in range(n):\n",
        "        diff_b += -(y[i] - sigmoid(w*x[i] + b))\n",
        "        diff_w += -(y[i] - sigmoid(w*x[i] + b))*x[i]\n",
        "    \n",
        "    # update\n",
        "    b -= learning_rate * diff_b\n",
        "    w -= learning_rate * diff_w\n",
        "\n",
        "print(\"beta's bias:\",b,\", weight:\",w)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "beta's bias: -15.32526844614968 , weight: 4.421877653019746\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}