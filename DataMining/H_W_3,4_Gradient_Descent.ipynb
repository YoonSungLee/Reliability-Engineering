{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "H.W #3,4. Gradient Descent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM/d6c9Ss8+SiQZ1k2I25OW"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHFZGc5-2v8g",
        "colab_type": "text"
      },
      "source": [
        "H.W #3: Logistic Regression에서 손실함수의 도함수가 아래와 같음을 보이시오."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZVAZfAG27ww",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://i.imgur.com/LYDqxId.png\" width=\"100%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pb3JRE-D3RLw",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://i.imgur.com/qTBMszv.jpg\" width=\"100%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tT_T0mhgEOl7",
        "colab_type": "text"
      },
      "source": [
        "H.W #4: 위의 도함수를 이용해 Gradient Descent 알고리즘으로 beta를 구하시오. 학습률 람다는 0.1로 하고 2000 회 반복 하강한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAV8Uxr7EVT1",
        "colab_type": "code",
        "outputId": "01f14d33-a2c9-43ed-fd29-1ac5ff3ee745",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "x = [1,2,3,4,5,6]\n",
        "y = [0,0,0,1,1,1]\n",
        "n = len(x)\n",
        "b = 3       # random initialization\n",
        "w = 5       # random initialization\n",
        "learning_rate = 0.1\n",
        "epoch = 2000\n",
        "\n",
        "for i in range(epoch):\n",
        "    diff_b = 0\n",
        "    diff_w = 0\n",
        "    for i in range(n):\n",
        "        diff_b += -(y[i] - sigmoid(w*x[i] + b))\n",
        "        diff_w += -(y[i] - sigmoid(w*x[i] + b))*x[i]\n",
        "    \n",
        "    # update\n",
        "    b -= learning_rate * diff_b\n",
        "    w -= learning_rate * diff_w\n",
        "\n",
        "print(\"beta's bias:\",b,\", weight:\",w)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "beta's bias: -15.32526844614968 , weight: 4.421877653019746\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1duvCLYSLAIC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "0ca65196-fa4a-41a7-b11c-647c909d42dc"
      },
      "source": [
        "# professor\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1+np.exp(-x))\n",
        "\n",
        "def deriv(w, b, x, y):\n",
        "    n = len(x)\n",
        "    summ_b = summ_w = 0\n",
        "    for i in range(n):\n",
        "        term = w * x[i] + b\n",
        "        summ_b += (y[i] - sigmoid(term))\n",
        "        summ_w += (y[i] - sigmoid(term)) * x[i]\n",
        "    return -summ_b, -summ_w\n",
        "\n",
        "\n",
        "\n",
        "x = [1,2,3,4,5,6]\n",
        "y = [0,0,0,1,1,1]\n",
        "\n",
        "b = w = 0.5\n",
        "lamda = 0.1\n",
        "\n",
        "for i in range(2000):\n",
        "    b = b - lamda*deriv(w, b, x, y)[0]\n",
        "    w = w - lamda*deriv(w, b, x, y)[1]\n",
        "\n",
        "b, w"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-15.512045717585206, 4.475786306086738)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qo6JxLcMzYh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "6d8910fa-a2b7-4819-d5de-dd1c4657e56e"
      },
      "source": [
        "# is it right?\n",
        "\n",
        "yhat = []\n",
        "\n",
        "for i in range(n):\n",
        "    yhat.append(sigmoid(b + w*x[i]))\n",
        "\n",
        "yhat"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.6106695250909766e-05,\n",
              " 0.0014132159996857042,\n",
              " 0.11059411673536196,\n",
              " 0.9161460735580977,\n",
              " 0.9989593679257665,\n",
              " 0.9999881440887478]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    }
  ]
}